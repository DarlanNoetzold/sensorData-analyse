# -*- coding: utf-8 -*-
"""Trabalho estatística.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cI1UbqfFMNkXkQBAWBS5yDnqStt8ALpl

# Testes iniciais
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

file_path = 'AirQuality.csv'
df = pd.read_csv(file_path, delimiter=';', skipinitialspace=True)

df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df = df.drop(columns=['NMHC(GT)'], errors='ignore')

df = df.replace(',', '.', regex=True)

cols_to_convert = df.columns[2:]
df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')

df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y', errors='coerce')

df = df.dropna(subset=['Date'])

df_cleaned = df.copy()
numeric_cols = df_cleaned.select_dtypes(include=['float64', 'int64']).columns

df_cleaned[numeric_cols] = df_cleaned[numeric_cols].apply(lambda x: x.mask(x < 0))

# Verificando se há mais valores negativos
print("Valores negativos ainda presentes:\n", (df_cleaned[numeric_cols] < 0).sum())

# Criar novas colunas de tempo: dia da semana, mês, semana do ano e trimestre
df['Day'] = df['Date'].dt.day
df['Weekday'] = df['Date'].dt.day_name()
df['Month'] = df['Date'].dt.month
df['Week'] = df['Date'].dt.isocalendar().week
df['Quarter'] = df['Date'].dt.quarter

# Aplicar a função de capping para limitar outliers
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns  # Apenas colunas numéricas

def cap_outliers(df, columns):
    df_capped = df.copy()
    for col in columns:
        Q1 = df_capped[col].quantile(0.25)
        Q3 = df_capped[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df_capped[col] = df_capped[col].clip(lower=lower_bound, upper=upper_bound)
    return df_capped

df_cleaned = cap_outliers(df, numeric_cols)

valid_numeric_cols = df_cleaned.select_dtypes(include=['float64', 'int64']).columns

# 1. Estatística Descritiva
print("\n1. Estatística Descritiva:")
desc_stats = df_cleaned[valid_numeric_cols].describe()
print(desc_stats)

# Gráficos de séries temporais para cada variável ao longo do tempo
def plot_time_series(df, variables):
    for var in variables:
        plt.figure(figsize=(12, 6))
        plt.plot(df['Date'], df[var], label=var)
        plt.title(f'Série Temporal de {var}')
        plt.xlabel('Date')
        plt.ylabel(var)
        plt.xticks(rotation=45)
        plt.legend()
        plt.show()

# Chamar a função para todas as variáveis numéricas
plot_time_series(df_cleaned, valid_numeric_cols)

# Gráficos de séries temporais ao longo de dias, dias da semana, semanas e meses
def plot_time_series_grouped(df, variables):
    periods = ['Day', 'Weekday', 'Week', 'Month']

    for var in variables:
        for period in periods:
            if df[var].dtype in ['float64', 'int64']:  # Certifique-se de que a coluna é numérica
                plt.figure(figsize=(12, 6))
                df_grouped = df.groupby(period).mean(numeric_only=True)[var]
                df_grouped.plot()
                plt.title(f'{var} ao longo de {period}')
                plt.xlabel(period)
                plt.ylabel(var)
                plt.xticks(rotation=45)
                plt.show()

# Chamar a função para todas as variáveis numéricas e diferentes séries temporais
plot_time_series_grouped(df_cleaned, valid_numeric_cols)

# Gráficos de histograma e boxplot para cada variável numérica
for col in valid_numeric_cols:
    plt.figure(figsize=(10, 4))

    # Remover valores NaN antes de plotar
    data_no_na = df_cleaned[col].dropna()

    plt.subplot(1, 2, 1)
    sns.histplot(data_no_na, kde=True)
    plt.title(f"Histograma de {col}")

    plt.subplot(1, 2, 2)
    sns.boxplot(y=data_no_na)
    plt.title(f"Boxplot de {col}")

    plt.tight_layout()
    plt.show()

# 2. Modelos de Distribuições Contínuas e Discretas
print("\n2. Modelos de Distribuições Contínuas e Discretas:")
# Exemplo com duas variáveis numéricas
plt.figure(figsize=(10, 5))
sns.histplot(df_cleaned['CO(GT)'].dropna(), kde=True, label='CO(GT)', color='blue')
sns.histplot(df_cleaned['T'].dropna(), kde=True, label='T', color='orange')
plt.title('Distribuições de CO(GT) e Temperatura')
plt.legend()
plt.show()

# 3. Correlação e Regressão
print("\n3. Correlação e Regressão:")
correlation_matrix = df_cleaned[valid_numeric_cols].corr()
print(correlation_matrix)

# Mapa de calor de correlação para todas as variáveis
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Mapa de Calor das Correlações')
plt.show()

# Regressão Linear Simples para todas as variáveis
for target_col in valid_numeric_cols:
    for predictor_col in valid_numeric_cols:
        if target_col != predictor_col:
            X = df_cleaned[[predictor_col]].dropna()
            y = df_cleaned[target_col].dropna()

            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            linear_model = LinearRegression()
            linear_model.fit(X_train, y_train)
            y_pred = linear_model.predict(X_test)

            print(f"R² da Regressão Linear entre {predictor_col} e {target_col}: {linear_model.score(X_test, y_test)}")
            print(f"Erro quadrático médio entre {predictor_col} e {target_col}: {mean_squared_error(y_test, y_pred)}")

            plt.figure(figsize=(10, 6))
            plt.scatter(X_test, y_test, label='Valores Reais')
            plt.plot(X_test, y_pred, color='red', label='Regressão Linear')
            plt.title(f'Regressão Linear entre {predictor_col} e {target_col}')
            plt.xlabel(predictor_col)
            plt.ylabel(target_col)
            plt.legend()
            plt.show()



# 4. Análise de Componentes Principais (PCA)
print("\n4. Análise de Componentes Principais (PCA):")
# Normalizando os dados antes de aplicar o PCA
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_cleaned[numeric_cols].dropna())

pca = PCA(n_components=2)
pca_result = pca.fit_transform(scaled_data)
print(f"Variância Explicada pelo PCA: {pca.explained_variance_ratio_}")

plt.figure(figsize=(8, 6))
plt.scatter(pca_result[:, 0], pca_result[:, 1], c=df_cleaned['NOx(GT)'], cmap='coolwarm')
plt.title('Análise de Componentes Principais (PCA)')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.show()

# 5. Amostragem e Métodos de Aleatorização
print("\n5. Amostragem e Métodos de Aleatorização:")
# Amostragem aleatória simples
sample_data = df_cleaned.sample(frac=0.1, random_state=42)
print("Amostragem Aleatória (10% dos dados):")
print(sample_data.head())

# 6. Métodos Multivariados
print("\n6. Métodos Multivariados:")
# Análise de pares (pairplot) para as variáveis numéricas
sns.pairplot(df_cleaned[numeric_cols].dropna(), kind='reg', diag_kind='kde')
plt.suptitle('Métodos Multivariados - Análise de Pares')
plt.show()

# 7. Machine Learning: Classificação Binária Simples
print("\n7. Machine Learning: Classificação Binária (Exemplo Simples):")
# Criando uma variável binária baseada em NO2(GT) para classificação
y = (df_cleaned['NO2(GT)'] > df_cleaned['NO2(GT)'].mean()).astype(int)
X = df_cleaned[numeric_cols].dropna()

# Dividindo os dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Utilizando RandomForestClassifier para a classificação binária
classifier = RandomForestClassifier(random_state=42)
classifier.fit(X_train, y_train)

# Avaliação do modelo
accuracy = classifier.score(X_test, y_test)
print(f"Acurácia do Modelo de Classificação: {accuracy * 100:.2f}%")

# Previsões
y_pred = classifier.predict(X_test)

# Matriz de Confusão
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nMatriz de Confusão:")
print(conf_matrix)

# Relatório de Classificação
class_report = classification_report(y_test, y_pred)
print("\nRelatório de Classificação:")
print(class_report)

from google.colab import drive
drive.mount('/content/drive')

# Função para identificar a estação do ano
def season(date):
    year = str(date.year)
    seasons = {'spring': pd.date_range(start='21/03/'+year, end='20/06/'+year),
               'summer': pd.date_range(start='21/06/'+year, end='22/09/'+year),
               'autumn': pd.date_range(start='23/09/'+year, end='20/12/'+year)}
    if date in seasons['spring']:
        return 'spring'
    if date in seasons['summer']:
        return 'summer'
    if date in seasons['autumn']:
        return 'autumn'
    else:
        return 'winter'

# Aplicando a função para criar uma nova coluna de estações
df_cleaned['Season'] = df_cleaned['Date'].map(season)

# Exibindo as primeiras 10 linhas para verificar a nova coluna
print(df_cleaned[['Date', 'Season']].head(10))

# Gráfico pairplot com separação por estação do ano
sns.pairplot(df_cleaned, hue='Season')
plt.show()

"""# Comparativo CO(GT) e PT08.S1(CO)

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from statistics import mode
import seaborn as sns

# 1. Carregar e Limpar os Dados
file_path = 'AirQuality.csv'
df = pd.read_csv(file_path, delimiter=';', skipinitialspace=True)
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df = df.replace(',', '.', regex=True)

cols_to_convert = df.columns[2:]
df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')
df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y', errors='coerce')
df = df.dropna(subset=['Date'])

# Remover linhas em que qualquer uma das colunas `CO(GT)` ou `PT08.S1(CO)` tenha valor -200
filtered_df = df[(df['CO(GT)'] != -200) & (df['PT08.S1(CO)'] != -200)].reset_index(drop=True)

# Calcular a moda de CO(GT) e filtrar dados onde CO(GT) = moda
try:
    moda_co = mode(filtered_df['CO(GT)'])
    print(f'A moda de CO(GT) é: {moda_co}')
except:
    print('Não foi possível calcular a moda de CO(GT).')
filtered_moda_df = filtered_df[filtered_df['CO(GT)'] == moda_co].reset_index(drop=True)

# 2. Análise de Erros
filtered_df['Erro Absoluto'] = abs(filtered_df['CO(GT)'] - filtered_df['PT08.S1(CO)'])
filtered_df['Erro Relativo (%)'] = (filtered_df['Erro Absoluto'] / filtered_df['CO(GT)']) * 100
print("Erro médio absoluto:", filtered_df['Erro Absoluto'].mean())
print("Erro médio relativo (%):", filtered_df['Erro Relativo (%)'].mean())

# 3. Gráfico de Bland-Altman
filtered_df['Média'] = (filtered_df['CO(GT)'] + filtered_df['PT08.S1(CO)']) / 2
filtered_df['Diferença'] = filtered_df['CO(GT)'] - filtered_df['PT08.S1(CO)']
mean_diff = filtered_df['Diferença'].mean()
std_diff = filtered_df['Diferença'].std()

plt.figure(figsize=(10, 6))
plt.scatter(filtered_df['Média'], filtered_df['Diferença'], alpha=0.5)
plt.axhline(mean_diff, color='red', linestyle='--', label='Média da Diferença')
plt.axhline(mean_diff + 1.96*std_diff, color='gray', linestyle='--', label='95% Limite Superior')
plt.axhline(mean_diff - 1.96*std_diff, color='gray', linestyle='--', label='95% Limite Inferior')
plt.xlabel('Média das Medições (CO(GT) e PT08.S1(CO))')
plt.ylabel('Diferença (CO(GT) - PT08.S1(CO))')
plt.legend()
plt.title('Diagrama de Bland-Altman')
plt.show()

# 4. Regressão Linear com Normalização
scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()
x = filtered_df[['CO(GT)']].values.reshape(-1, 1)
y = filtered_df['PT08.S1(CO)'].values.reshape(-1, 1)

x_normalized = scaler_x.fit_transform(x)
y_normalized = scaler_y.fit_transform(y)

correlation = np.corrcoef(x_normalized[:, 0], y_normalized[:, 0])[0, 1]
print(f"Correlação entre CO(GT) e PT08.S1(CO) (normalizada): {correlation:.2f}")

# Ajuste da regressão linear
model = LinearRegression()
model.fit(x_normalized, y_normalized)
slope = model.coef_[0][0]
intercept = model.intercept_[0]
print(f"Modelo de regressão linear (normalizado): y = {slope:.2f} * x + {intercept:.2f}")

# Plot da regressão linear normalizada
plt.figure(figsize=(10, 6))
plt.scatter(x_normalized, y_normalized, color='blue', label='Dados Reais Normalizados')
plt.plot(x_normalized, model.predict(x_normalized), color='red', linewidth=2, label='Regressão Linear')
plt.xlabel('CO(GT) (Normalizado)')
plt.ylabel('PT08.S1(CO) (Normalizado)')
plt.title('Regressão Linear entre CO(GT) e PT08.S1(CO) (Normalizado)')
plt.legend()
plt.show()

# 5. Teste de Hipóteses para Diferenças
t_stat, p_val = stats.ttest_rel(filtered_df['CO(GT)'], filtered_df['PT08.S1(CO)'])
print(f"Teste t pareado: Estatística t = {t_stat}, p-valor = {p_val}")

# 6. Verificar normalidade das diferenças com Teste Shapiro-Wilk
shapiro_test = stats.shapiro(filtered_df['Diferença'].dropna())
print(f"Teste de Shapiro-Wilk para normalidade das diferenças: p-valor = {shapiro_test.pvalue}")
if shapiro_test.pvalue < 0.05:
    print("As diferenças não são normais. Considere um teste não paramétrico como o teste de Wilcoxon.")

# 7. Intervalo de Confiança para a Média das Diferenças
mean_diff_conf_int = stats.t.interval(0.95, len(filtered_df['Diferença'])-1, loc=mean_diff, scale=std_diff/np.sqrt(len(filtered_df['Diferença'])))
print(f"Intervalo de Confiança de 95% para a Média das Diferenças: {mean_diff_conf_int}")

# 8. Bootstrapping para Intervalo de Confiança das Diferenças
bootstrap_samples = 1000
bootstrap_means = []
np.random.seed(42)

for _ in range(bootstrap_samples):
    sample = filtered_df['Diferença'].dropna().sample(frac=1, replace=True)
    bootstrap_means.append(sample.mean())

# Cálculo dos percentis para intervalo de confiança de 95%
conf_int_bootstrap = np.percentile(bootstrap_means, [2.5, 97.5])
print(f"Intervalo de Confiança de 95% (Bootstrapping) para a Média das Diferenças: {conf_int_bootstrap}")

# Plot do histograma dos resultados do bootstrapping
plt.figure(figsize=(10, 6))
plt.hist(bootstrap_means, bins=30, color='skyblue', edgecolor='black')
plt.axvline(conf_int_bootstrap[0], color='red', linestyle='--', label='Limite Inferior 95%')
plt.axvline(conf_int_bootstrap[1], color='red', linestyle='--', label='Limite Superior 95%')
plt.title("Distribuição dos Erros Médios (Bootstrapping)")
plt.xlabel("Erro Médio")
plt.ylabel("Frequência")
plt.legend()
plt.show()

"""Os resultados das análises com o tratamento de dados agora mostram detalhes mais completos sobre a relação entre o sensor PT08 e o equipamento de referência CO(GT):

### Moda de CO(GT):

* A moda de CO(GT) é 1.0, o que representa o valor mais frequente registrado pelo equipamento de referência.

### Erro Médio Absoluto e Relativo:

* O erro médio absoluto entre o sensor e o equipamento de referência é 1108.45, indicando uma diferença significativa em média.

* O erro médio relativo é extremamente alto, cerca de 79764.99%, mostrando uma grande discrepância entre as leituras do sensor e do equipamento de referência.

### Gráfico de Bland-Altman:

* O gráfico de Bland-Altman mostra uma tendência linear descendente, indicando que o sensor PT08 tende a subestimar os valores em relação ao CO(GT). Essa subestimação aumenta à medida que o valor médio entre CO(GT) e PT08.S1(CO) aumenta.

### Regressão Linear (Normalizada):

* A correlação entre CO(GT) e PT08.S1(CO) normalizados é 0.88, o que indica uma correlação moderadamente forte entre as medições.
* O modelo de regressão linear normalizado é dado por y=1.13×x+0.14, sugerindo que o sensor PT08 acompanha parcialmente o CO(GT), mas com um desvio significativo.

### Teste de Hipóteses (Teste t Pareado):

* O teste t pareado mostra uma estatística t de -436.90 com um p-valor de 0.0, indicando uma diferença significativa entre as medições do sensor e do equipamento de referência.

### Teste de Normalidade das Diferenças (Shapiro-Wilk):

* O teste Shapiro-Wilk produziu um p-valor muito baixo (2.17e-39), indicando que as diferenças entre CO(GT) e PT08.S1(CO) não seguem uma distribuição normal.

### Intervalo de Confiança (Método t e Bootstrapping):

* O intervalo de confiança para a média das diferenças, calculado pelo método t e por bootstrapping, é aproximadamente entre -1113.42 e -1103.48, o que reforça que o sensor subestima consistentemente os valores do CO(GT).

### Conclusão
Essas análises indicam que o sensor PT08.S1(CO) apresenta uma subestimação consistente e significativa em relação ao equipamento de referência CO(GT). A diferença entre as medições é alta e estatisticamente significativa, e o erro relativo extremo sugere que o sensor pode não ser adequado para capturar com precisão os valores de CO(GT).

# Análise Série temporal
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.tsa.stattools import ccf
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

# Carregar e preparar os dados
file_path = 'AirQuality.csv'
df = pd.read_csv(file_path, delimiter=';', skipinitialspace=True)

# Remover colunas indesejadas e converter para formato numérico
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df = df.replace(',', '.', regex=True)
cols_to_convert = df.columns[2:]
df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')
df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y', errors='coerce')
df = df.dropna(subset=['Date'])

# Limpeza adicional e remoção de outliers
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
df[numeric_cols] = df[numeric_cols].apply(lambda x: x.mask(x < 0))
df = df[(df['CO(GT)'] != -200) & (df['PT08.S1(CO)'] != -200)]

# Criar novas colunas temporais
df['Day'] = df['Date'].dt.day
df['Weekday'] = df['Date'].dt.day_name()
df['Month'] = df['Date'].dt.month
df['Week'] = df['Date'].dt.isocalendar().week
df['Quarter'] = df['Date'].dt.quarter

# Preenchimento de valores ausentes para a decomposição da série temporal
df['CO(GT)'] = df['CO(GT)'].interpolate()
df['PT08.S1(CO)'] = df['PT08.S1(CO)'].interpolate()

# Decomposição da Série Temporal para CO(GT) e PT08.S1(CO)
co_gt_series = df.set_index('Date')['CO(GT)'].resample('D').mean()
pt08_series = df.set_index('Date')['PT08.S1(CO)'].resample('D').mean()

result_gt = seasonal_decompose(co_gt_series.dropna(), model='additive', period=30)
result_gt.plot()
plt.suptitle("Decomposição da Série Temporal - CO(GT)")
plt.show()

result_pt08 = seasonal_decompose(pt08_series.dropna(), model='additive', period=30)
result_pt08.plot()
plt.suptitle("Decomposição da Série Temporal - PT08.S1(CO)")
plt.show()

# Análise de Autocorrelação
plot_acf(co_gt_series.dropna(), lags=30)
plt.title("Autocorrelação - CO(GT)")
plt.show()

plot_acf(pt08_series.dropna(), lags=30)
plt.title("Autocorrelação - PT08.S1(CO)")
plt.show()

# Correlação Cruzada entre CO(GT) e PT08.S1(CO)
lags = 20
plt.stem(range(-lags, lags+1), ccf(co_gt_series.dropna(), pt08_series.dropna(), adjusted=False)[:2*lags+1])
plt.xlabel('Lag')
plt.ylabel('Correlação Cruzada')
plt.title("Correlação Cruzada entre CO(GT) e PT08.S1(CO)")
plt.show()

# Modelagem e Previsão com ARIMA
# Ajuste do modelo ARIMA para CO(GT)
model_gt = ARIMA(co_gt_series.dropna(), order=(1, 1, 1))
model_fit_gt = model_gt.fit()
forecast_gt = model_fit_gt.forecast(steps=30)

# Ajuste do modelo ARIMA para PT08.S1(CO)
model_pt08 = ARIMA(pt08_series.dropna(), order=(1, 1, 1))
model_fit_pt08 = model_pt08.fit()
forecast_pt08 = model_fit_pt08.forecast(steps=30)

# Visualização das previsões
plt.figure(figsize=(12, 6))
plt.plot(co_gt_series, label='CO(GT) Histórico')
plt.plot(forecast_gt, label='Previsão CO(GT)', color='red')
plt.legend()
plt.title("Previsão de CO(GT) com ARIMA")
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(pt08_series, label='PT08.S1(CO) Histórico')
plt.plot(forecast_pt08, label='Previsão PT08.S1(CO)', color='orange')
plt.legend()
plt.title("Previsão de PT08.S1(CO) com ARIMA")
plt.show()

# Avaliação da precisão das previsões
rmse_gt = np.sqrt(mean_squared_error(co_gt_series[-30:], forecast_gt))
rmse_pt08 = np.sqrt(mean_squared_error(pt08_series[-30:], forecast_pt08))
print(f"RMSE para CO(GT): {rmse_gt}")
print(f"RMSE para PT08.S1(CO): {rmse_pt08}")

# Visualizações de padrões sazonais
def plot_time_series_grouped(df, variable, period):
    plt.figure(figsize=(12, 6))
    df_grouped = df.groupby(period).mean(numeric_only=True)[variable]
    df_grouped.plot()
    plt.title(f'{variable} ao longo de {period}')
    plt.xlabel(period)
    plt.ylabel(variable)
    plt.xticks(rotation=45)
    plt.show()

# Exibir padrões ao longo do tempo para diferentes colunas e períodos
for var in ['CO(GT)', 'PT08.S1(CO)']:
    for period in ['Weekday', 'Month', 'Quarter']:
        plot_time_series_grouped(df, var, period)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Função para identificar a estação do ano
def season(date):
    year = str(date.year)
    seasons = {
        'spring': pd.date_range(start='21/03/'+year, end='20/06/'+year),
        'summer': pd.date_range(start='21/06/'+year, end='22/09/'+year),
        'autumn': pd.date_range(start='23/09/'+year, end='20/12/'+year)
    }
    if date in seasons['spring']:
        return 'spring'
    elif date in seasons['summer']:
        return 'summer'
    elif date in seasons['autumn']:
        return 'autumn'
    else:
        return 'winter'

# Aplicar a função para criar uma nova coluna de estações
df['Season'] = df['Date'].map(season)

# Exibir as primeiras linhas para verificar a nova coluna
print(df[['Date', 'Season']].head(10))

# Estatísticas descritivas agrupadas por estação
print("\nEstatísticas descritivas por estação:")
seasonal_stats = df.groupby('Season')[['CO(GT)', 'PT08.S1(CO)']].describe()
print(seasonal_stats)

# Verificar colunas disponíveis para o pairplot
available_columns = [col for col in ['CO(GT)', 'PT08.S1(CO)', 'NO2(GT)'] if col in df.columns]

# Visualização pairplot das variáveis por estação do ano
if available_columns:
    sns.pairplot(df, hue='Season', vars=available_columns)
    plt.suptitle("Distribuição das Medições por Estação do Ano")
    plt.show()
else:
    print("Nenhuma coluna válida para plotagem encontrada.")

# Gráficos de média sazonal ao longo do ano para CO(GT) e PT08.S1(CO)
def plot_seasonal_means(df, variable):
    plt.figure(figsize=(10, 6))
    df_seasonal = df.groupby(['Season', 'Month']).mean(numeric_only=True).reset_index()
    sns.barplot(data=df_seasonal, x='Month', y=variable, hue='Season')
    plt.title(f'Média Mensal de {variable} por Estação do Ano')
    plt.xlabel('Mês')
    plt.ylabel(variable)
    plt.legend(title='Estação')
    plt.show()

# Plot para CO(GT) e PT08.S1(CO) se as colunas existirem no dataframe
if 'CO(GT)' in df.columns:
    plot_seasonal_means(df, 'CO(GT)')
if 'PT08.S1(CO)' in df.columns:
    plot_seasonal_means(df, 'PT08.S1(CO)')

"""# Nova análise - Do zero"""

import pandas as pd
import numpy as np

# Load dataset
data = pd.read_csv('AirQuality.csv', delimiter=';', decimal=',')

# Display initial data information to understand the structure and contents
data_info = data.info()
data_head = data.head()  # Display the first few rows for inspection

data_info, data_head

"""O dataset contém 9471 entradas com 17 colunas, incluindo colunas Date e Time, além de valores de concentração para diferentes poluentes (ex., CO, NMHC, NOx) tanto a partir de sensores quanto de um analisador de referência. Notei duas colunas (Unnamed: 15 e Unnamed: 16) com valores nulos, que podem ser descartadas para limpeza dos dados.

Vou proceder com o tratamento dos dados ausentes e normalização das colunas de concentração dos sensores para garantir comparabilidade entre eles e o analisador de referência. Primeiro, vamos excluir as colunas desnecessárias e substituir valores faltantes. Em seguida, aplicaremos as normalizações.
"""

# Drop unnecessary columns with all null values
data = data.drop(columns=['Unnamed: 15', 'Unnamed: 16'])

# Replace missing values (-200) with NaN for proper handling
data = data.replace(-200, np.nan)

# Perform Min-Max normalization on relevant columns
# Separating sensor and reference analyzer columns for specific handling
sensor_columns = ['PT08.S1(CO)', 'PT08.S2(NMHC)', 'PT08.S3(NOx)', 'PT08.S4(NO2)', 'PT08.S5(O3)']
reference_columns = ['CO(GT)', 'NMHC(GT)', 'C6H6(GT)', 'NOx(GT)', 'NO2(GT)']

# Min-Max normalization for sensors
data[sensor_columns] = (data[sensor_columns] - data[sensor_columns].min()) / (data[sensor_columns].max() - data[sensor_columns].min())

# Z-score normalization for reference analyzer values (assuming these have larger variations)
data[reference_columns] = (data[reference_columns] - data[reference_columns].mean()) / data[reference_columns].std()

# Display the first few rows to confirm the transformations
data.head()

"""Os dados foram limpos e normalizados com sucesso. Para a normalização:

As colunas dos sensores (PT08.S1(CO), PT08.S2(NMHC), etc.) foram normalizadas pelo método Min-Max, trazendo seus valores para uma escala entre 0 e 1.
As colunas do analisador de referência (CO(GT), NMHC(GT), etc.) foram padronizadas usando Z-score, resultando em valores com média 0 e desvio padrão 1.

Prosseguindo, a próxima etapa é aplicar as metodologias estatísticas conforme os problemas identificados: validação do novo dispositivo, identificação de padrões temporais e previsão de qualidade do ar. Vamos dividir e aplicar técnicas específicas para cada um desses objetivos, abordando uma análise estatística e visual para cada problema.

### 3.1 Validação do Novo Dispositivo
Para validar a precisão do dispositivo multisensor em comparação com o analisador de referência, realizaremos:

Correlação: Avaliando a correlação entre cada sensor e sua variável correspondente do analisador de referência para entender o nível de associação linear.
Regressão Linear: Modelando a relação entre cada sensor e sua variável de referência com regressão linear, para quantificar o ajuste entre as respostas do sensor e os valores de referência.
Teste T para Duas Amostras: Testando se as médias das respostas dos sensores e as concentrações de referência diferem significativamente.
Vamos começar pela análise de correlação entre os sensores e as variáveis do analisador de referência.
"""

# Calculate correlation between sensors and their corresponding reference variables
correlations = {}
for sensor, reference in zip(sensor_columns, reference_columns):
    correlations[f'{sensor} vs {reference}'] = data[sensor].corr(data[reference])

# Convert correlation results to a DataFrame for better readability
correlation_df = pd.DataFrame(list(correlations.items()), columns=['Sensor-Reference Pair', 'Correlation'])

correlation_df

"""Os coeficientes de correlação foram calculados para cada par sensor-referência, mostrando o grau de associação linear entre os sensores e os valores do analisador de referência. A correlação próxima de 1 ou -1 indica uma associação linear forte, enquanto valores mais próximos de 0 indicam uma relação fraca ou inexistente.

Em seguida, realizaremos a análise de regressão linear para quantificar o ajuste entre cada sensor e sua variável de referência e, posteriormente, aplicaremos o teste T para avaliar a significância das diferenças nas médias. Vamos começar pela análise de regressão.
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Prepare results container for regression analysis
regression_results = []

# Perform linear regression for each sensor-reference pair
for sensor, reference in zip(sensor_columns, reference_columns):
    # Drop NaN values for regression analysis
    sensor_data = data[[sensor, reference]].dropna()
    X = sensor_data[[sensor]].values.reshape(-1, 1)  # Sensor data as independent variable
    y = sensor_data[reference].values  # Reference data as dependent variable

    # Initialize and fit the regression model
    model = LinearRegression()
    model.fit(X, y)

    # Predict and calculate R-squared for model fit quality
    predictions = model.predict(X)
    r_squared = r2_score(y, predictions)

    # Store slope, intercept, and R-squared
    regression_results.append({
        'Sensor-Reference Pair': f'{sensor} vs {reference}',
        'Slope': model.coef_[0],
        'Intercept': model.intercept_,
        'R-squared': r_squared
    })

# Convert regression results to DataFrame for readability
regression_df = pd.DataFrame(regression_results)

regression_df

"""A análise de regressão linear revelou o coeficiente angular (slope), a interceptação e o coeficiente de determinação R2 para cada par sensor-referência. O R2 indica a qualidade do ajuste: valores próximos de 1 representam um ajuste forte, enquanto valores baixos sugerem que o sensor não consegue explicar bem a variabilidade dos dados de referência. Por exemplo, o par PT08.S1(CO) vs CO(GT) apresenta um R2 relativamente alto, indicando uma boa relação entre o sensor e o valor de referência.

O próximo passo será aplicar o Teste T para Duas Amostras para verificar a significância das diferenças entre as médias de cada sensor e sua concentração de referência. Vamos a essa análise.
"""

from scipy.stats import ttest_ind

# Prepare results container for T-test results
t_test_results = []

# Perform T-test for each sensor-reference pair
for sensor, reference in zip(sensor_columns, reference_columns):
    # Drop NaN values for test
    sensor_data = data[sensor].dropna()
    reference_data = data[reference].dropna()

    # Perform T-test
    t_stat, p_value = ttest_ind(sensor_data, reference_data, equal_var=False)

    # Store T-test results
    t_test_results.append({
        'Sensor-Reference Pair': f'{sensor} vs {reference}',
        'T-statistic': t_stat,
        'P-value': p_value
    })

# Convert T-test results to DataFrame for readability
t_test_df = pd.DataFrame(t_test_results)

t_test_df

"""Os resultados do Teste T indicam que há uma diferença estatisticamente significativa entre as médias dos sensores e os valores de referência, pois os valores de p são extremamente baixos para todos os pares. Isso sugere que, embora haja correlação e certo nível de ajuste nos modelos de regressão, os sensores e o analisador de referência apresentam variações consideráveis.

o teste T é mais adequado para verificar se há uma diferença significativa entre as médias de duas amostras assumindo independência. Aqui, estamos lidando com dados temporais e contínuos de dois dispositivos que deveriam medir a mesma variável e não amostras independentes, o que faz com que o teste T possa não ser a melhor escolha.

Vou refazer a análise de validação dos sensores com métodos mais apropriados para verificar a concordância e consistência entre os sensores e o analisador de referência. Abaixo está a justificativa e o novo plano de análise:

Justificativa para os Novos Métodos
Ao invés do teste T, que se destina a comparar médias de amostras independentes, vamos utilizar as seguintes métricas de avaliação de desempenho e concordância:

Erro Médio Absoluto (MAE) e Raiz do Erro Quadrático Médio (RMSE): Estas métricas de erro medem a diferença média e quadrática média entre os valores dos sensores e as concentrações de referência, fornecendo uma avaliação quantitativa de quão próximos os sensores estão dos valores de referência.

Coeficiente de Concordância de Correlação de Lin (CCC) ou Análise de Bland-Altman: Estes métodos são amplamente usados para comparar a concordância entre dois dispositivos de medição. O CCC mede a precisão e acurácia combinadas, enquanto a análise de Bland-Altman verifica a extensão das discrepâncias entre as medições.
"""

# Re-import and preprocess the dataset for alignment
data = pd.read_csv('AirQuality.csv', delimiter=';', decimal=',')
data = data.drop(columns=['Unnamed: 15', 'Unnamed: 16'])
data = data.replace(-200, np.nan)

# Define sensor and reference columns
sensor_columns = ['PT08.S1(CO)', 'PT08.S2(NMHC)', 'PT08.S3(NOx)', 'PT08.S4(NO2)', 'PT08.S5(O3)']
reference_columns = ['CO(GT)', 'NMHC(GT)', 'C6H6(GT)', 'NOx(GT)', 'NO2(GT)']

# Min-Max normalize all columns for alignment
all_columns = sensor_columns + reference_columns
data[all_columns] = (data[all_columns] - data[all_columns].min()) / (data[all_columns].max() - data[all_columns].min())

# Calculate MAE, RMSE, and create Bland-Altman plots
error_results_aligned = []  # Container for MAE and RMSE results
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 10))
axes = axes.flatten()

for i, (sensor, reference) in enumerate(zip(sensor_columns, reference_columns)):
    # Drop NaN values and align sensor and reference data
    aligned_data = data[[sensor, reference]].dropna()
    sensor_data = aligned_data[sensor]
    reference_data = aligned_data[reference]

    # Calculate MAE and RMSE
    mae = mean_absolute_error(reference_data, sensor_data)
    rmse = mean_squared_error(reference_data, sensor_data, squared=False)
    error_results_aligned.append({
        'Sensor-Reference Pair': f'{sensor} vs {reference}',
        'MAE': mae,
        'RMSE': rmse
    })

    # Bland-Altman plot
    mean_values = (sensor_data + reference_data) / 2
    differences = sensor_data - reference_data
    mean_diff = differences.mean()
    std_diff = differences.std()

    # Plotting
    ax = axes[i]
    ax.scatter(mean_values, differences, alpha=0.5)
    ax.axhline(mean_diff, color='red', linestyle='--', label='Mean Difference')
    ax.axhline(mean_diff + 1.96 * std_diff, color='blue', linestyle='--', label='95% Limits')
    ax.axhline(mean_diff - 1.96 * std_diff, color='blue', linestyle='--')
    ax.set_title(f'Bland-Altman Plot: {sensor} vs {reference}')
    ax.set_xlabel('Mean of Sensor and Reference')
    ax.set_ylabel('Difference (Sensor - Reference)')
    ax.legend()

plt.tight_layout()
plt.show()

# Convert error results to DataFrame for display
error_aligned_df = pd.DataFrame(error_results_aligned)
error_aligned_df

"""Os resultados foram gerados com sucesso:

MAE e RMSE: As métricas de erro foram calculadas para cada par sensor-referência, com valores mais baixos indicando maior precisão. PT08.S5(O3) vs NO2(GT) apresentou os menores valores de erro, enquanto PT08.S4(NO2) vs NOx(GT) apresentou os maiores.

Gráficos de Bland-Altman: Exibem a diferença entre cada sensor e sua referência, com a linha média e os limites de concordância de 95%. A maioria dos pontos está dentro dos limites, especialmente para pares com menores valores de erro.
"""

# Plot scatter plots with regression lines for each sensor-reference pair

fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 10))
axes = axes.flatten()

for i, (sensor, reference) in enumerate(zip(sensor_columns, reference_columns)):
    # Drop NaN values and align sensor and reference data
    aligned_data = data[[sensor, reference]].dropna()
    sensor_data = aligned_data[sensor]
    reference_data = aligned_data[reference]

    # Scatter plot
    ax = axes[i]
    ax.scatter(sensor_data, reference_data, alpha=0.5, label='Data points')

    # Regression line
    X = sensor_data.values.reshape(-1, 1)
    model = LinearRegression()
    model.fit(X, reference_data.values)
    predictions = model.predict(X)

    # Plot regression line
    ax.plot(sensor_data, predictions, color='red', linestyle='--', label='Regression line')

    # Title and labels
    ax.set_title(f'Scatter Plot and Regression: {sensor} vs {reference}')
    ax.set_xlabel(sensor)
    ax.set_ylabel(reference)
    ax.legend()

plt.tight_layout()
plt.show()

""" Esses gráficos ajudam a visualizar a correlação e a qualidade do ajuste, facilitando a análise da relação linear entre as medições do sensor e os valores de referência:

A linha de regressão em cada gráfico mostra a direção e o ajuste da relação entre o sensor e a referência.
Nos pares com maior correlação, como PT08.S1(CO) vs CO(GT) e PT08.S2(NMHC) vs NMHC(GT), observa-se uma linha de regressão mais próxima dos pontos, indicando uma relação mais forte e precisa.
"""

import seaborn as sns

# Calculate the correlation matrix for all sensor and reference columns
correlation_matrix = data[sensor_columns + reference_columns].corr()

# Plot the heatmap for correlation
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", cbar=True)
plt.title("Correlation Matrix for Sensor and Reference Columns")
plt.show()

"""Esse gráfico permite visualizar de maneira clara a força das associações entre cada par sensor-referência, com tons mais intensos indicando correlações fortes (positivas ou negativas) e tons neutros indicando associações mais fracas.

Esse heatmap complementa bem a análise, destacando pares com alta correlação, como PT08.S1(CO) vs CO(GT) e PT08.S2(NMHC) vs NMHC(GT), ajudando a validar visualmente as relações entre os dados dos sensores e as referências.

### 3.2 Identificação de Padrões Temporais
Para identificar padrões temporais nos níveis de poluição ao longo de diferentes períodos, foram aplicadas as seguintes técnicas:

* Análise Temporal e Sazonalidade: A análise de médias e variações ao longo de diferentes estações do ano, dias da semana e horas do dia permitiu identificar os horários e períodos de maior e menor poluição. Séries temporais foram divididas em períodos diários e sazonais para capturar flutuações, como horários de pico durante o dia e variações climáticas durante o ano.
* Métodos de Distribuições Discretas: A distribuição de Poisson foi utilizada para modelar a frequência de eventos específicos, como altos níveis de poluição, ao longo de diferentes períodos. Essa técnica ajudou a identificar a probabilidade de ocorrência de níveis elevados de poluição em horários específicos, permitindo uma avaliação da variabilidade ao longo do tempo.
* Análise de Componentes Principais (PCA): A PCA foi aplicada para reduzir a dimensionalidade dos dados temporais e identificar as principais variáveis que influenciam as variações nos níveis de poluição. Com a redução de variáveis, foi possível visualizar quais poluentes contribuem mais para as mudanças sazonais e temporais na qualidade do ar.
"""

from sklearn.decomposition import PCA
# Convert 'Date' column to datetime format and extract temporal features for grouping
data['Date'] = pd.to_datetime(data['Date'], errors='coerce', dayfirst=True)
data['Time'] = pd.to_datetime(data['Time'], format='%H.%M.%S', errors='coerce').dt.hour  # Extract hour

# Extract day, month, and season for analysis
data['DayOfWeek'] = data['Date'].dt.dayofweek  # Monday=0, Sunday=6
data['Month'] = data['Date'].dt.month
data['Season'] = data['Month'] % 12 // 3 + 1  # 1=Winter, 2=Spring, 3=Summer, 4=Autumn

# Prepare data for plotting by grouping
time_group = data.groupby('Time')[reference_columns].mean()
day_group = data.groupby('DayOfWeek')[reference_columns].mean()
month_group = data.groupby('Month')[reference_columns].mean()
season_group = data.groupby('Season')[reference_columns].mean()

# Plot average concentration by hour of day
time_group.plot(figsize=(10, 6))
plt.title('Average Concentration by Hour of Day')
plt.xlabel('Hour')
plt.ylabel('Concentration (normalized)')
plt.legend(title='Pollutants')
plt.show()

# Plot average concentration by day of week
day_group.plot(figsize=(10, 6))
plt.title('Average Concentration by Day of Week')
plt.xlabel('Day of Week (0=Monday)')
plt.ylabel('Concentration (normalized)')
plt.legend(title='Pollutants')
plt.show()

# Plot average concentration by month
month_group.plot(figsize=(10, 6))
plt.title('Average Concentration by Month')
plt.xlabel('Month')
plt.ylabel('Concentration (normalized)')
plt.legend(title='Pollutants')
plt.show()

# Plot average concentration by season
season_group.plot(kind='bar', figsize=(10, 6))
plt.title('Average Concentration by Season')
plt.xlabel('Season (1=Winter, 2=Spring, 3=Summer, 4=Autumn)')
plt.ylabel('Concentration (normalized)')
plt.legend(title='Pollutants')
plt.show()


print("POISSON")

# 2. Model high pollution events using Poisson distribution
# Define a threshold for high pollution events (e.g., 75th percentile)
high_pollution_threshold = data[reference_columns].quantile(0.75)
high_pollution_events = data[reference_columns].ge(high_pollution_threshold).astype(int)

# Calculate the frequency of high pollution events by hour and day of week
hourly_pollution_events = high_pollution_events.groupby(data['Time']).sum()
daily_pollution_events = high_pollution_events.groupby(data['DayOfWeek']).sum()

# Plot the hourly high pollution events (frequency of high pollution levels per hour)
hourly_pollution_events.plot(kind='bar', figsize=(12, 6))
plt.title('Frequency of High Pollution Events by Hour')
plt.xlabel('Hour of Day')
plt.ylabel('Number of High Pollution Events')
plt.legend(title='Pollutants')
plt.show()

# Plot the daily high pollution events (frequency of high pollution levels per day of the week)
daily_pollution_events.plot(kind='bar', figsize=(12, 6))
plt.title('Frequency of High Pollution Events by Day of Week')
plt.xlabel('Day of Week (0=Monday)')
plt.ylabel('Number of High Pollution Events')
plt.legend(title='Pollutants')
plt.show()

# 3. Apply PCA to the dataset for temporal variation analysis
# Dropping non-numeric columns and handling missing values for PCA
pca_data = data[reference_columns].dropna()

# Initialize and fit PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(pca_data)

# Convert PCA result to DataFrame for easy plotting and interpretation
pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])

# Plot the PCA results
plt.figure(figsize=(10, 6))
plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.5)
plt.title('PCA of Pollution Levels')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True)
plt.show()

# Display explained variance ratio for each principal component
explained_variance = pca.explained_variance_ratio_
explained_variance

"""Os gráficos acima mostram a média de concentração de poluentes em diferentes intervalos temporais:

1. Hora do Dia: Observa-se uma variação nos níveis de poluição ao longo do dia, possivelmente relacionada a horários de pico e atividades urbanas.

2. Dia da Semana: A análise dos dias da semana destaca variações sutis que podem estar ligadas ao aumento da atividade nos dias úteis em comparação aos finais de semana.

3. Mês: Esse gráfico ajuda a identificar mudanças sazonais, como picos de poluição em meses específicos, que podem estar associados a condições climáticas e padrões de circulação de veículos.

4. Estação: Ao agrupar por estação (inverno, primavera, verão, outono), observamos padrões mais amplos, com certas estações apresentando maiores concentrações de poluentes, possivelmente devido a fatores climáticos que afetam a dispersão dos poluentes.

## Distribuição de Poisson
A Distribuição de Poisson é usada para modelar a frequência de eventos raros em um intervalo de tempo fixo. No contexto de poluição, isso se traduz em modelar a ocorrência de eventos de alta poluição em determinados períodos.

Passos:
1. Definição de Evento de Alta Poluição: Consideramos um "evento de alta poluição" quando a concentração de um poluente está acima do 75º percentil. Esse valor serve como um limite para definir níveis elevados de poluição.

2. Contagem dos Eventos: Para cada intervalo de tempo (horas do dia e dias da semana), contei a frequência de eventos de alta poluição para cada poluente. Isso nos ajuda a observar quando esses eventos são mais prováveis de ocorrer ao longo do dia ou da semana.

Resultados:
Os gráficos de barras mostram a frequência de eventos de alta poluição em diferentes horas e dias. Isso ilustra quando ocorrem mais eventos elevados de poluição, como picos em horários de alta atividade urbana (ex.: manhãs e tardes de dias úteis).

### Análise de Componentes Principais (PCA)
A Análise de Componentes Principais (PCA) reduz a dimensionalidade dos dados, identificando quais variáveis têm maior influência na variabilidade observada. Aplicamos o PCA para entender quais poluentes contribuem mais para as mudanças temporais na poluição.

Passos:
1. Preparação dos Dados: Utilizei as colunas de concentração dos poluentes e excluí valores ausentes para garantir que o PCA funcionasse corretamente.

2. Aplicação do PCA: Extraí os dois principais componentes (PC1 e PC2), que representam as direções de maior variabilidade nos dados. O PCA gera combinações lineares das variáveis originais (poluentes), mostrando quais contribuem mais para a variabilidade.

3. Explained Variance: O PCA indica que o primeiro componente principal (PC1) explica 90.8% da variação, enquanto o segundo componente (PC2) explica 6.4%. Isso significa que uma grande parte da variabilidade nos níveis de poluição pode ser explicada por esses dois componentes.

Resultados:
O gráfico de dispersão do PCA nos ajuda a visualizar a relação entre esses componentes e observar quais poluentes influenciam mais na variabilidade ao longo do tempo. Para identificar os fatores representados por PC1 e PC2, precisamos analisar os pesos (ou "loadings") de cada variável (poluente) nos componentes principais. Esses pesos indicam a contribuição de cada variável para os componentes, permitindo entender quais poluentes mais influenciam cada direção de variabilidade.
"""

# Get the PCA component loadings (weights of each variable in each principal component)
loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=reference_columns)

# Display the loadings for interpretation
loadings

"""Aqui estão os pesos de cada variável (poluente) nos componentes principais:

### PC1:

* Os maiores pesos são atribuídos a NMHC(GT), CO(GT), e C6H6(GT), sugerindo que este componente está fortemente associado à variabilidade nas concentrações de hidrocarbonetos não metânicos, monóxido de carbono e benzeno.
* Esses poluentes são comuns em áreas urbanas com tráfego intenso, o que indica que PC1 representa a variabilidade associada às emissões veiculares e atividades industriais.

### PC2:

* NMHC(GT) e NO2(GT) têm os maiores pesos em PC2, mas com sinais opostos (positivo para NMHC(GT) e negativo para NO2(GT)), sugerindo que este componente reflete uma relação inversa entre esses poluentes.
* Isso pode indicar que PC2 captura variações relacionadas a condições atmosféricas ou processos químicos que afetam a dispersão e a transformação dos poluentes, como a formação de ozônio e interações entre hidrocarbonetos e óxidos de nitrogênio.

Essas interpretações ajudam a entender que PC1 está mais associado às fontes de emissão diretas (como tráfego), enquanto PC2 pode refletir a influência de processos atmosféricos e dispersão dos poluentes na região. ​

### 3.3 Previsão de Qualidade do Ar
Para essa fase, vamos seguir os seguintes passos:

* Preparação dos Dados: Usaremos as variáveis temporais e concentrações de poluentes já processadas, dividindo os dados em conjuntos de treino e teste.

* Seleção de Modelos de Machine Learning: Testaremos modelos como Regressão Linear, Árvores de Decisão e Florestas Aleatórias para comparar o desempenho.
Treinamento e Avaliação dos Modelos:

*  Avaliaremos cada modelo com métricas como o Erro Médio Absoluto (MAE) e a Raiz do Erro Quadrático Médio (RMSE), para verificar a precisão das previsões.
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression

# Prepare data by selecting target variable and features
# Target variable: NO2(GT) (can choose others for comparison as needed)
target = 'NO2(GT)'
features = [col for col in reference_columns if col != target] + ['Time', 'DayOfWeek', 'Month', 'Season']

# Drop NaN values from features and target columns
data_ml = data.dropna(subset=features + [target])

# Define X (features) and y (target) for machine learning
X = data_ml[features]
y = data_ml[target]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(random_state=42)
}

# Train and evaluate each model
results = []
for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Predict on test data
    y_pred = model.predict(X_test)

    # Calculate MAE and RMSE
    mae = mean_absolute_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    # Store results
    results.append({
        'Model': model_name,
        'MAE': mae,
        'RMSE': rmse
    })

# Convert results to DataFrame for readability
results_df = pd.DataFrame(results)
results_df

"""Aqui estão os resultados da previsão de qualidade do ar com os três modelos testados:

Linear Regression:

MAE: 0.0316
RMSE: 0.0395
Decision Tree:

MAE: 0.0253
RMSE: 0.0319
Random Forest:

MAE: 0.0216
RMSE: 0.0276
O modelo Random Forest obteve os menores valores de erro (MAE e RMSE), indicando melhor precisão e ajuste. Esse modelo conseguiu captar melhor as relações complexas nos dados temporais e nas concentrações de poluentes.
"""

# Predict on the training data to visualize training performance
y_train_pred = models['Random Forest'].predict(X_train)

# Plot training results
plt.figure(figsize=(12, 6))
plt.plot(y_train.values, label='Real (Train)', alpha=0.6)
plt.plot(y_train_pred, label='Predicted (Train)', alpha=0.6)
plt.title("Training Data: Real vs Predicted Values")
plt.xlabel("Sample Index")
plt.ylabel("Concentration (Normalized)")
plt.legend()
plt.show()

# Predict on the test data to visualize test performance
y_test_pred = models['Random Forest'].predict(X_test)

# Plot test results
plt.figure(figsize=(12, 6))
plt.plot(y_test.values, label='Real (Test)', alpha=0.6)
plt.plot(y_test_pred, label='Predicted (Test)', alpha=0.6)
plt.title("Test Data: Real vs Predicted Values")
plt.xlabel("Sample Index")
plt.ylabel("Concentration (Normalized)")
plt.legend()
plt.show()

# Simulate future predictions using a time series approach
# Sort test data by original time order for a time series forecast comparison
X_test_sorted = X_test.sort_index()
y_test_sorted = y_test.sort_index()
y_test_pred_sorted = models['Random Forest'].predict(X_test_sorted)

# Plot time series predictions vs real values
plt.figure(figsize=(12, 6))
plt.plot(y_test_sorted.values, label='Real (Time Series)', alpha=0.6)
plt.plot(y_test_pred_sorted, label='Predicted (Time Series)', alpha=0.6)
plt.title("Time Series Prediction: Real vs Predicted Values")
plt.xlabel("Time Step")
plt.ylabel("Concentration (Normalized)")
plt.legend()
plt.show()

"""Os gráficos mostram o desempenho do modelo Random Forest nos conjuntos de treinamento e teste, além de uma comparação temporal:

Conjunto de Treinamento: Os valores previstos seguem de perto os valores reais, indicando que o modelo foi bem ajustado e captou os padrões nos dados de treino.

Conjunto de Teste: As previsões também mantêm uma correspondência sólida com os valores reais, confirmando que o modelo consegue generalizar bem para novos dados.

Série Temporal (Ordenada): Na simulação de previsão ao longo de uma sequência temporal, o modelo conseguiu acompanhar as flutuações dos dados reais, indicando boa capacidade de previsão para padrões de poluição ao longo do tempo.

Esses resultados demonstram que o modelo pode ser útil para previsão contínua da qualidade do ar.
"""

